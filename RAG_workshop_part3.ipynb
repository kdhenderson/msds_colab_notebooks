{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kdhenderson/msds_colab_notebooks/blob/main/RAG_workshop_part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtNaQBX18lQy"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "The following code is designed to run without any issues, as I have ensured all the packages are compatible. The primary function of this code is to read a PDF file and respond to any questions by providing relevant text from the file along with the source.\n",
        "\n",
        "As an example, the code reads an academic paper on the impacts of greenspace on health, and answers the question: \"Is cardiovascular mortality affected by greenspace exposure?\"\n",
        "\n",
        "__Improving Chunking Strategies:__ Using sentences and paragraphs for chunking ensures better context preservation and more meaningful chunks.\n",
        "\n",
        "__Using More Sophisticated Language Models:__ Utilizing larger, more powerful models can generate better embeddings, improving the retrieval accuracy.\n",
        "\n",
        "__Refining the Answer Generation Process:__ Combining multiple top chunks and generating a refined answer ensures a more comprehensive and accurate response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm9S55RU8lQ0",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pip install PyMuPDF transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCWUIYlY8lQ1",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade gradio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "A-zAE4dq40lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pip install urllib3==1.26.12\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Qg6tolTQ4hsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests==2.28.2"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PsypROGl4z-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-whnR_e8lQ1"
      },
      "source": [
        "## OpenAI Text Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93s9T6w98lQ2",
        "outputId": "e0314d0d-44c2-41f0-8b47-905b833e30f5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Answer: Each case will have a point value between 10 and 50 points. Generally you can expect a total of 150-200 points depending on the\n",
            "Sources: ['ds_6371_syllabus Ver 7.pdf, Chunk 13', 'ds_6371_syllabus Ver 7.pdf, Chunk 12', 'ds_6371_syllabus Ver 7.pdf, Chunk 20']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import openai\n",
        "import torch\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 1: Read PDF Files\n",
        "def read_pdfs(folder_path):\n",
        "    pdf_texts = []\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith('.pdf'):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            try:\n",
        "                doc = fitz.open(file_path)\n",
        "                text = \"\"\n",
        "                for page in doc:\n",
        "                    text += page.get_text()\n",
        "                pdf_texts.append((file_name, text))\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {file_name}: {e}\")\n",
        "    return pdf_texts\n",
        "\n",
        "# Step 2: Chunk Text\n",
        "def chunk_text(text, chunk_size=100):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        if current_length + len(words) > chunk_size:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "        current_chunk.extend(words)\n",
        "        current_length += len(words)\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Step 3: Create Embeddings\n",
        "def create_embeddings(text_chunks, tokenizer, model):\n",
        "    embeddings = []\n",
        "    for chunk in text_chunks:\n",
        "        inputs = tokenizer(chunk, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# Step 4: Index Embeddings\n",
        "def index_embeddings(embeddings):\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "# Step 5: Answer Questions using OpenAI API\n",
        "def answer_question_openai(question, pdf_texts, index, embeddings, tokenizer, model, openai_api_key, temperature=1.0, max_tokens=150, top_k=3):\n",
        "    openai.api_key = openai_api_key\n",
        "\n",
        "    # Create embedding for the question\n",
        "    inputs = tokenizer(question, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        question_embedding = model(**inputs).last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "    # Search for the nearest text chunks\n",
        "    _, indices = index.search(np.array([question_embedding]), k=top_k)\n",
        "    indices = indices[0]\n",
        "\n",
        "    # Collect top-k chunks\n",
        "    retrieved_chunks = []\n",
        "    sources = []\n",
        "    for idx in indices:\n",
        "        chunk_offset = idx\n",
        "        pdf_idx = 0\n",
        "\n",
        "        while chunk_offset >= len(pdf_texts[pdf_idx][1]):\n",
        "            chunk_offset -= len(pdf_texts[pdf_idx][1])\n",
        "            pdf_idx += 1\n",
        "\n",
        "        pdf_name, chunks = pdf_texts[pdf_idx]\n",
        "        retrieved_chunks.append(chunks[chunk_offset])\n",
        "        sources.append(f\"{pdf_name}, Chunk {chunk_offset}\")\n",
        "\n",
        "    # Combine retrieved chunks\n",
        "    combined_text = ' '.join(retrieved_chunks)\n",
        "\n",
        "    # Prepare the prompt for OpenAI\n",
        "    prompt = f\"Question: {question}\\n\\nContext: {combined_text}\\n\\nAnswer:\"\n",
        "\n",
        "    # Call OpenAI API\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"davinci-002\",  # or any other GPT-3 model\n",
        "        prompt=prompt,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        n=1,\n",
        "        stop=None\n",
        "    )\n",
        "\n",
        "    refined_answer = response.choices[0].text.strip()\n",
        "\n",
        "    return f\"Answer: {refined_answer}\\nSources: {sources}\"\n",
        "\n",
        "# Main function to tie everything together\n",
        "def main(folder_path, question, model_name, openai_api_key, temperature=1.0, max_tokens=150):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    # Read and chunk PDFs\n",
        "    pdf_texts = read_pdfs(folder_path)\n",
        "    all_chunks = []\n",
        "    chunk_mapping = []\n",
        "\n",
        "    for pdf_name, text in pdf_texts:\n",
        "        chunks = chunk_text(text)\n",
        "        all_chunks.extend(chunks)\n",
        "        chunk_mapping.append((pdf_name, chunks))\n",
        "\n",
        "    # Create and index embeddings\n",
        "    embeddings = create_embeddings(all_chunks, tokenizer, model)\n",
        "    index = index_embeddings(embeddings)\n",
        "\n",
        "    # Answer question using OpenAI\n",
        "    answer = answer_question_openai(question, chunk_mapping, index, embeddings, tokenizer, model, openai_api_key, temperature, max_tokens)\n",
        "    print(answer)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    folder_path = '/content/drive/My Drive/PDFs/'  # Adjust this to your folder path\n",
        "    question = \"What is the homework worth in DS 6371?\"\n",
        "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # or any other model you prefer\n",
        "    openai_api_key = \"XYZ123\"  # Replace with your actual OpenAI API key\n",
        "    temperature = .9\n",
        "    max_tokens = 30\n",
        "    main(folder_path, question, model_name, openai_api_key, temperature, max_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYZ4a6yE8lQ3"
      },
      "source": [
        "# Chat Model from OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruAfdA-p8lQ3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import openai\n",
        "import torch\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Step 1: Read PDF Files\n",
        "def read_pdfs(folder_path):\n",
        "    pdf_texts = []\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith('.pdf'):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            try:\n",
        "                doc = fitz.open(file_path)\n",
        "                text = \"\"\n",
        "                for page in doc:\n",
        "                    text += page.get_text()\n",
        "                pdf_texts.append((file_name, text))\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {file_name}: {e}\")\n",
        "    return pdf_texts\n",
        "\n",
        "# Step 2: Chunk Text\n",
        "def chunk_text(text, chunk_size=100):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        if current_length + len(words) > chunk_size:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "        current_chunk.extend(words)\n",
        "        current_length += len(words)\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Step 3: Create Embeddings\n",
        "def create_embeddings(text_chunks, tokenizer, model):\n",
        "    embeddings = []\n",
        "    for chunk in text_chunks:\n",
        "        inputs = tokenizer(chunk, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# Step 4: Index Embeddings\n",
        "def index_embeddings(embeddings):\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "# Step 5: Answer Questions using OpenAI Chat API\n",
        "def answer_question_openai_chat(question, pdf_texts, index, embeddings, tokenizer, model, openai_api_key, temperature=1.0, max_tokens=150, top_k=6):\n",
        "    openai.api_key = openai_api_key\n",
        "\n",
        "    # Create embedding for the question\n",
        "    inputs = tokenizer(question, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        question_embedding = model(**inputs).last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "    # Search for the nearest text chunks\n",
        "    _, indices = index.search(np.array([question_embedding]), k=top_k)\n",
        "    indices = indices[0]\n",
        "\n",
        "    # Collect top-k chunks\n",
        "    retrieved_chunks = []\n",
        "    sources = []\n",
        "    for idx in indices:\n",
        "        chunk_offset = idx\n",
        "        pdf_idx = 0\n",
        "\n",
        "        while chunk_offset >= len(pdf_texts[pdf_idx][1]):\n",
        "            chunk_offset -= len(pdf_texts[pdf_idx][1])\n",
        "            pdf_idx += 1\n",
        "\n",
        "        pdf_name, chunks = pdf_texts[pdf_idx]\n",
        "        retrieved_chunks.append(chunks[chunk_offset])\n",
        "        sources.append(f\"{pdf_name}, Chunk {chunk_offset}\")\n",
        "\n",
        "    # Combine retrieved chunks\n",
        "    combined_text = ' '.join(retrieved_chunks)\n",
        "\n",
        "    # Prepare the messages for the Chat API\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that is reading a sylabus for a student.  Be breif when possible.  You don't need use all the tokens in your response. Always start the response by saying a kind greeting.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Context: {combined_text}\\n\\nQuestion: {question}\"}\n",
        "    ]\n",
        "\n",
        "    # Call OpenAI Chat API\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",  # or any other available chat model\n",
        "        messages=messages,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "\n",
        "    refined_answer = response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "#Call1 REason RAG\n",
        "\n",
        "#Go look up text from Text resolution for that call1 reason\n",
        "\n",
        "#send that text from the test resolution and the orsignial X (2000 words of text) to a final seq2seq model to provide the text the customer.\n",
        "\n",
        "    return f\"Answer: {refined_answer}\\nSources: {sources}\"\n",
        "\n",
        "# Main function to tie everything together\n",
        "def main(folder_path, question, model_name, openai_api_key, temperature=1.0, max_tokens=150):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    # Read and chunk PDFs\n",
        "    pdf_texts = read_pdfs(folder_path)\n",
        "    all_chunks = []\n",
        "    chunk_mapping = []\n",
        "\n",
        "    for pdf_name, text in pdf_texts:\n",
        "        chunks = chunk_text(text)\n",
        "        all_chunks.extend(chunks)\n",
        "        chunk_mapping.append((pdf_name, chunks))\n",
        "\n",
        "    # Create and index embeddings\n",
        "    embeddings = create_embeddings(all_chunks, tokenizer, model)\n",
        "    index = index_embeddings(embeddings)\n",
        "\n",
        "    # Answer question using OpenAI Chat API\n",
        "    answer = answer_question_openai_chat(question, chunk_mapping, index, embeddings, tokenizer, model, openai_api_key, temperature, max_tokens)\n",
        "    print(answer)\n",
        "\n",
        "\n",
        "\n",
        "# Gradio Interface Functions\n",
        "def process_pdfs_and_answer_question(folder_path, question, model_name, openai_api_key, temperature=0.1, max_tokens=150):\n",
        "\n",
        "    #os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    # Read and chunk PDFs\n",
        "    pdf_texts = read_pdfs(folder_path)\n",
        "    all_chunks = []\n",
        "    chunk_mapping = []\n",
        "\n",
        "    for pdf_name, text in pdf_texts:\n",
        "        chunks = chunk_text(text)\n",
        "        all_chunks.extend(chunks)\n",
        "        chunk_mapping.append((pdf_name, chunks))\n",
        "\n",
        "    # Create and index embeddings\n",
        "    embeddings = create_embeddings(all_chunks, tokenizer, model)\n",
        "    index = index_embeddings(embeddings)\n",
        "\n",
        "    # Answer question using OpenAI\n",
        "    answer = answer_question_openai_chat(question, chunk_mapping, index, embeddings, tokenizer, model, openai_api_key, temperature, max_tokens)\n",
        "    return answer\n",
        "\n",
        "# Create Gradio Interface\n",
        "iface = gr.Interface(\n",
        "    fn=process_pdfs_and_answer_question,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, placeholder=\"RAG Folder Path\", label=\"Path\"),\n",
        "        gr.Textbox(lines=2, placeholder=\"Enter your question here...\", label=\"Question\"),\n",
        "        gr.Textbox(label=\"Model Name\"),\n",
        "        gr.Textbox(type=\"password\", label=\"OpenAI API Key\"),\n",
        "        gr.Slider(minimum=0.0, maximum=1.0, label=\"Temperature\"),\n",
        "        gr.Slider(minimum=1, maximum=256, label=\"Max Tokens\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"PDF Question Answering System\",\n",
        "    description=\"Upload PDF files, ask questions, and get answers based on the content of the PDFs.\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpTPaXJZ8lQ6"
      },
      "source": [
        "## Temperature\n",
        "\"Temperature\" is a parameter used in the generation process of large language models (LLMs) to control the randomness of the output. It essentially adjusts the probability distribution of the predicted tokens, influencing the diversity and creativity of the generated text. Here's how it works:\n",
        "\n",
        "Low Temperature (e.g., close to 0): When the temperature is low, the model becomes more deterministic and conservative. It tends to choose the most probable next word in the sequence, leading to more predictable and repetitive outputs. This setting is useful when you want the model to generate precise and factual text.\n",
        "\n",
        "High Temperature (e.g., 1.0 or higher): When the temperature is high, the model's predictions become more random and diverse. The probability distribution is flattened, meaning less likely words have a higher chance of being selected. This can lead to more creative and varied responses, but it can also introduce more mistakes or less coherent text.\n",
        "\n",
        "In essence, the temperature parameter allows you to balance between coherence and creativity:\n",
        "\n",
        "Low temperature: More focused and deterministic output.\n",
        "High temperature: More diverse and creative output.\n",
        "The choice of temperature depends on the specific use case and the desired characteristics of the generated text.\n",
        "\n",
        "Here's an example illustrating the effect of temperature on a simple prompt:\n",
        "\n",
        "Prompt: \"The quick brown fox\"\n",
        "\n",
        "Low temperature (e.g., 0.2):\n",
        "Output: \"The quick brown fox jumps over the lazy dog.\"\n",
        "High temperature (e.g., 1.0):\n",
        "Output: \"The quick brown fox danced under a glowing moon, chasing shadows.\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is For Reference inputs to gradio above"
      ],
      "metadata": {
        "id": "WjmpnyYsJQG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#folder_path = '/content/drive/My Drive/PDFs/'  # Adjust this to your folder path\n",
        "#    question = \"What is the SMU Honor Code?\"\n",
        "#    question = \"What is the FLS assignment?\"\n",
        "#    question = \"What is homework worth in DS 6371?\"\n",
        "#    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # or any other model you prefer\n",
        "#    openai_api_key = \"XYZ123\"  # Replace with your actual OpenAI API key\n",
        "#    temperature = .1\n",
        "#    max_tokens = 20\n",
        "#    main(folder_path, question, model_name, openai_api_key, temperature, max_tokens)"
      ],
      "metadata": {
        "id": "5bInDtb43laB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "environmentMetadata": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "RAG - Added LLM Test",
      "widgets": {}
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}