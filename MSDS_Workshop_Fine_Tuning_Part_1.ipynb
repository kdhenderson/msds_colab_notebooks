{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kdhenderson/msds_colab_notebooks/blob/main/MSDS_Workshop_Fine_Tuning_Part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 1. Install Dependencies\n",
        "!pip install -q unsloth bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install -q sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n"
      ],
      "metadata": {
        "id": "i2MoJg3fBTJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 2. Import Libraries\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "import torch\n",
        "import json"
      ],
      "metadata": {
        "id": "OSnzjVN-BUcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 3. Set Parameters\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True"
      ],
      "metadata": {
        "id": "Jg_ISyY5BWdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 4. Load Base Model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "id": "h8-F3pZCBXvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 5. Add LoRA Adapters (required for 4-bit finetuning)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 8,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16,  # how much weight do you want to put on the new matrix vs pretrained (bigger more weight on fine-tuned data)\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 42,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "VHhPa92mBg_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 6. Create Example Train + Test JSONL Files\n",
        "train_data = [\n",
        "    {\"conversations\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is the HW percentage in DS 6371?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The homework percentage in 6371 is 10%\"}\n",
        "    ]},\n",
        "    {\"conversations\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is the midterm percentage in DS 6371?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The midterm is worth 25% in DS 6371\"}\n",
        "    ]},\n",
        "    {\"conversations\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is the Final Exam worth in DS6371\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The final exam is worth 25% of the grade in DS 6371\"}\n",
        "    ]},\n",
        "    {\"conversations\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is the HW percentage in DS 6371?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The homework percentage in 6371 is 10%\"}\n",
        "    ]},\n",
        "    {\"conversations\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is the midterm percentage in DS 6371?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The midterm is worth 25% in DS 6371\"}\n",
        "    ]},\n",
        "    {\"conversations\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is the Final Exam worth in DS6371\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The final exam is worth 25% of the grade in DS 6371\"}\n",
        "    ]},\n",
        "        {\"conversations\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is the HW percentage in DS 6371?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The homework percentage in 6371 is 10%\"}\n",
        "    ]},\n",
        "    {\"conversations\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is the midterm percentage in DS 6371?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The midterm is worth 25% in DS 6371\"}\n",
        "    ]},\n",
        "    {\"conversations\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is the Final Exam worth in DS6371\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The final exam is worth 25% of the grade in DS 6371\"}\n",
        "    ]}\n",
        "]\n",
        "\n",
        "test_data = [\n",
        "    {\"conversations\": [\n",
        "        {\"role\": \"user\", \"content\": \"What percentage of the grade is the homework worth in DS6371?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"10%\"}\n",
        "    ]},\n",
        "    {\"conversations\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is 5 + 7?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"5 + 7 equals 12.\"}\n",
        "    ]},\n",
        "    {\"conversations\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is the midterm percentage in DS 6371?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"25%\"}\n",
        "    ]},\n",
        "    {\"conversations\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is the weight of the final exam in DS 6371?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"It means Excitement, Respect and Celebration of Hard Work.\"}\n",
        "    ]},\n",
        "    {\"conversations\": [\n",
        "        {\"role\": \"user\", \"content\": \"What does Whamo mean?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"It means Excitement, Respect and Celebration of Hard Work.\"}\n",
        "    ]}\n",
        "]\n",
        "\n",
        "with open(\"train.jsonl\", \"w\") as f:\n",
        "    for item in train_data:\n",
        "        f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "with open(\"test.jsonl\", \"w\") as f:\n",
        "    for item in test_data:\n",
        "        f.write(json.dumps(item) + \"\\n\")"
      ],
      "metadata": {
        "id": "nxgcwuvYBjcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 7. Load and Format Dataset\n",
        "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.1\")\n",
        "\n",
        "def format_conversations(example):\n",
        "    text = tokenizer.apply_chat_template(example[\"conversations\"], tokenize=False, add_generation_prompt=False)\n",
        "    return {\"text\": text}\n",
        "\n",
        "train_ds = load_dataset(\"json\", data_files=\"train.jsonl\", split=\"train\").map(format_conversations)\n",
        "test_ds = load_dataset(\"json\", data_files=\"test.jsonl\", split=\"train\").map(format_conversations)"
      ],
      "metadata": {
        "id": "qcIV1OD9Boxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 8. Tokenize and Mask Responses\n",
        "train_ds = train_ds.map(lambda x: tokenizer(x[\"text\"]), batched=True, num_proc=2)\n",
        "test_ds = test_ds.map(lambda x: tokenizer(x[\"text\"]), batched=True, num_proc=2)"
      ],
      "metadata": {
        "id": "AfdRNz1EBrHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 9. Trainer Config\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_ds,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer),\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        max_steps=40,    # epochs\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"adamw_8bit\",\n",
        "        seed=42,\n",
        "        report_to=\"none\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "i8Mwm5A_BthP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 10. Mask User Inputs, Only Train on Assistant Outputs\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "g0LvY21ZBvcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ✅ 11. Train Model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "Da3XMAmGByBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 12. Inference on Test Data\n",
        "from transformers import TextStreamer\n",
        "FastLanguageModel.for_inference(model)  # Enable faster inference\n",
        "\n",
        "for example in test_data:\n",
        "    messages = example[\"conversations\"][:1]  # Just the user message\n",
        "    inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(inputs, max_new_tokens=100, temperature=0.99, top_p=0.9)\n",
        "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "tjDpBylGB0BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcN6MGN49MnV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    }
  ]
}