{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kdhenderson/msds_colab_notebooks/blob/main/RAG_workshop_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTDQK1Xwae9Y"
      },
      "source": [
        "# Retrieval Augmented Generation\n",
        "## Part 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6wCtLnzamHu"
      },
      "source": [
        "#Step 0: Install and import useful packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18RlsTrPKaNp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# PyMuPDF -> digest pdfs; tranformers -> hugging face models; faiss-cpu (facebook pkg) -> vectorize\n",
        "pip install PyMuPDF transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQkUzJA-Kn5q",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%pip install nltk  # natural language toolkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fG36i_oLmn3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import faiss\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eWiuTQMacoC"
      },
      "source": [
        "# Step 1: Read PDF Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqXIeKFuMWxg",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Example folder path in Google Drive\n",
        "folder_path = '/content/drive/My Drive/PDFs/'  # Adjust this to your folder path\n",
        "#file_path = '/content/drive/MyDrive/documents/my_pdf_file.pdf'\n",
        "\n",
        "def read_pdfs(folder_path):\n",
        "    pdf_texts = []\n",
        "    for file_name in os.listdir(folder_path):  # can put many pdfs in here (will slow it down)\n",
        "        if file_name.endswith('.pdf'):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            try:\n",
        "                doc = fitz.open(file_path)  # fitz function (digest pdfs)\n",
        "                text = \"\"\n",
        "                for page in doc:\n",
        "                    text += page.get_text()\n",
        "                pdf_texts.append((file_name, text))\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {file_name}: {e}\")\n",
        "    return pdf_texts\n",
        "\n",
        "# Run the function\n",
        "pdf_contents = read_pdfs(folder_path)\n",
        "\n",
        "# Display the results\n",
        "for file_name, text in pdf_contents:\n",
        "    print(f\"Contents of {file_name}:\\n{text[:1000]}...\")  # Display first 100 characters for preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD7pF3TaTBVY",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pdf_texts = read_pdfs(folder_path)\n",
        "\n",
        "pdf_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXhH_SVsaUdj"
      },
      "source": [
        "# Step 2: Chunk Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olQQQ_giTVZS"
      },
      "outputs": [],
      "source": [
        "# Step 2: Chunk Text\n",
        "def chunk_text(text, chunk_size=100):  # chunk_size = hyperparameter (can't be more than 100 tokens, i.e. ~words)\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        if current_length + len(words) > chunk_size:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "        current_chunk.extend(words)\n",
        "        current_length += len(words)\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "   # Print out each chunk\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        print(f\"Chunk {i}: {chunk}\\n\")\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1IT1Q8GUJgz",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt_tab')\n",
        "\n",
        "all_chunks = []\n",
        "chunk_mapping = []\n",
        "\n",
        "for pdf_name, text in pdf_texts:\n",
        "    chunks = chunk_text(text)\n",
        "    all_chunks.extend(chunks)\n",
        "    chunk_mapping.append((pdf_name, chunks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBpLnw0raSYb"
      },
      "source": [
        "# Step 3: Create Embeddings / Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kxc0wa-pUbk2"
      },
      "outputs": [],
      "source": [
        "# Step 3: Create Embeddings\n",
        "def create_embeddings(text_chunks, tokenizer, model):\n",
        "    embeddings = []\n",
        "    for chunk in text_chunks:\n",
        "        inputs = tokenizer(chunk, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
        "\n",
        "          # Print out each embedding\n",
        "    for i, embed in enumerate(embeddings):\n",
        "        print(f\"Embedding {i}: {embed}\\n\")\n",
        "\n",
        "    return np.array(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeWBiinWUh2j",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 sentence transformer model to 384 dim vector\n",
        "model = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "model = AutoModel.from_pretrained(model)\n",
        "\n",
        "# Create embeddings\n",
        "embeddings = create_embeddings(all_chunks, tokenizer, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yySWYkVfaAQ7"
      },
      "source": [
        "# Step 4: Index Vectors / Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZJULca-aCGU"
      },
      "source": [
        "Indexing embeddings allows for efficient retrieval of relevant text chunks. Without indexing, finding similar chunks would involve comparing the query embedding against all embeddings, which is computationally expensive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJxUfkTJUuJD"
      },
      "outputs": [],
      "source": [
        "# Step 4: Index Embeddings\n",
        "def index_embeddings(embeddings):\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "# faiss vectorization strategy (organize based on semantic values, cosine similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JR-kcJPUyZx",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        " # Index embeddings\n",
        " index = index_embeddings(embeddings)\n",
        " index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCICIWuVZ2Oe"
      },
      "source": [
        "# Step 5: Retrieve and return relevant chunks.\n",
        "### Note that there is no LLM to provide a refined answer here... we were add this later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55XdRWQSU6VF"
      },
      "outputs": [],
      "source": [
        "# Step 5: Answer Questions\n",
        "def answer_question(question, pdf_texts, index, embeddings, tokenizer, model, top_k=3):\n",
        "    # Create embedding for the question\n",
        "    inputs = tokenizer(question, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        question_embedding = model(**inputs).last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "    # Search for the nearest text chunks\n",
        "    _, indices = index.search(np.array([question_embedding]), k=top_k)\n",
        "    indices = indices[0]\n",
        "\n",
        "    # Collect top-k chunks\n",
        "    retrieved_chunks = []\n",
        "    sources = []\n",
        "    for idx in indices:\n",
        "        chunk_offset = idx\n",
        "        pdf_idx = 0\n",
        "\n",
        "        while chunk_offset >= len(pdf_texts[pdf_idx][1]):\n",
        "            chunk_offset -= len(pdf_texts[pdf_idx][1])\n",
        "            pdf_idx += 1\n",
        "\n",
        "        pdf_name, chunks = pdf_texts[pdf_idx]\n",
        "        retrieved_chunks.append(chunks[chunk_offset])\n",
        "        sources.append(f\"{pdf_name}, Chunk {chunk_offset}\")\n",
        "\n",
        "\n",
        "\n",
        "    combined_text = ' '.join(retrieved_chunks)\n",
        "    return f\"Answer: {combined_text}\\nSources: {sources}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flFfX4DQVQp0",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        " # Answer question\n",
        "question = \"What percent of the overall grade is the homework grade worth in DS 6371?\"\n",
        "answer = answer_question(question, chunk_mapping, index, embeddings, tokenizer, model, top_k=3)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHTc0zEjZpd-"
      },
      "source": [
        "# All Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kmqgEw5ViwZ"
      },
      "outputs": [],
      "source": [
        "# Step 1: Read PDF Files\n",
        "def read_pdfs(folder_path):\n",
        "    pdf_texts = []\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith('.pdf'):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            try:\n",
        "                doc = fitz.open(file_path)\n",
        "                text = \"\"\n",
        "                for page in doc:\n",
        "                    text += page.get_text()\n",
        "                pdf_texts.append((file_name, text))\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {file_name}: {e}\")\n",
        "    return pdf_texts\n",
        "\n",
        "# Step 2: Chunk Text\n",
        "def chunk_text(text, chunk_size=100):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        if current_length + len(words) > chunk_size:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "        current_chunk.extend(words)\n",
        "        current_length += len(words)\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Step 3: Create Embeddings\n",
        "def create_embeddings(text_chunks, tokenizer, model):\n",
        "    embeddings = []\n",
        "    for chunk in text_chunks:\n",
        "        inputs = tokenizer(chunk, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# Step 4: Index Embeddings\n",
        "def index_embeddings(embeddings):\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "# Step 5: Answer Questions\n",
        "def answer_question(question, pdf_texts, index, embeddings, tokenizer, model, top_k=3):\n",
        "    # Create embedding for the question\n",
        "    inputs = tokenizer(question, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        question_embedding = model(**inputs).last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "    # Search for the nearest text chunks\n",
        "    _, indices = index.search(np.array([question_embedding]), k=top_k)\n",
        "    indices = indices[0]\n",
        "\n",
        "    # Collect top-k chunks\n",
        "    retrieved_chunks = []\n",
        "    sources = []\n",
        "    for idx in indices:\n",
        "        chunk_offset = idx\n",
        "        pdf_idx = 0\n",
        "\n",
        "        while chunk_offset >= len(pdf_texts[pdf_idx][1]):\n",
        "            chunk_offset -= len(pdf_texts[pdf_idx][1])\n",
        "            pdf_idx += 1\n",
        "\n",
        "        pdf_name, chunks = pdf_texts[pdf_idx]\n",
        "        retrieved_chunks.append(chunks[chunk_offset])\n",
        "        sources.append(f\"{pdf_name}, Chunk {chunk_offset}\")\n",
        "\n",
        "\n",
        "\n",
        "    combined_text = ' '.join(retrieved_chunks)\n",
        "    return f\"Answer: {combined_text}\\nSources: {sources}\"\n",
        "\n",
        "\n",
        "# Main function to tie everything together\n",
        "def main(folder_path, question, model):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    model = AutoModel.from_pretrained(model)\n",
        "\n",
        "    # Read and chunk PDFs\n",
        "    pdf_texts = read_pdfs(folder_path)\n",
        "    all_chunks = []\n",
        "    chunk_mapping = []\n",
        "\n",
        "    for pdf_name, text in pdf_texts:\n",
        "        chunks = chunk_text(text)\n",
        "        all_chunks.extend(chunks)\n",
        "        chunk_mapping.append((pdf_name, chunks))\n",
        "\n",
        "    # Create and index embeddings\n",
        "    embeddings = create_embeddings(all_chunks, tokenizer, model)\n",
        "    index = index_embeddings(embeddings)\n",
        "\n",
        "    # Answer question\n",
        "    answer = answer_question(question, chunk_mapping, index, embeddings, tokenizer, model)\n",
        "    print(answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-CZgAtlZnFs"
      },
      "source": [
        "# Comparing Different Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDBoazPxV0xt"
      },
      "outputs": [],
      "source": [
        "#question = 'What does the \"Check drainage\" code mean on the washer?'\n",
        "#question = 'What is Campus Caring Connections?'\n",
        "question = \"What percent of the overall grade is the homework grade worth in DS 6371?\"\n",
        "#question = \"What determines the  largest percent of the grade?\"\n",
        "#question = \"What is the FLS assignment?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4zQY740YTS8"
      },
      "source": [
        "__DistilBERT Variants__\n",
        "  - __distilbert-base-uncased:__ A distilled version of the original BERT model, which is optimized for speed and reduced size, while retaining much of the performance of the larger BERT models.\n",
        "  - __distilroberta-base:__ A distilled version of the RoBERTa model, offering similar benefits in terms of size and speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEc1h7m_V998",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "main(folder_path, question, 'distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rA2bbK9Wm06",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "main(folder_path, question, 'distilroberta-base')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj0CfMXFYJ1i"
      },
      "source": [
        "__BERT Variants:__\n",
        "  - __bert-large-uncased:__ A larger version of BERT with more parameters, which can provide better embeddings and improved performance.\n",
        "  - __roberta-large:__ A robustly optimized BERT approach with more parameters and improved training techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vRjjg27WtIT",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "main(folder_path, question, 'bert-large-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZZJbIfwX2Ne"
      },
      "source": [
        "__Sentence Transformers:__\n",
        "\n",
        "  - __all-MiniLM-L6-v2:__ A lightweight model optimized for generating sentence embeddings efficiently.\n",
        "  - __all-mpnet-base-v2:__ A variant of MPNet optimized for generating high-quality sentence embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SKkmpiGW5YE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "main(folder_path, question, 'sentence-transformers/all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ze_wshuEXEar",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "main(folder_path, question, 'sentence-transformers/all-mpnet-base-v2')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}