{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kdhenderson/msds_colab_notebooks/blob/main/MSDS_Workshop_Fine_Tuning_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "s3MnCVG0JWB7"
      },
      "outputs": [],
      "source": [
        "# âœ… 1. Install required packages\n",
        "!pip install -q unsloth bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install -q sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… 2. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "bQ_WZ-e80p0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jAHWC-QJc3S"
      },
      "outputs": [],
      "source": [
        "\n",
        "# âœ… 3. Import and load model\n",
        "from unsloth import FastLanguageModel\n",
        "max_seq_length = 2048\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "# âœ… 4. Apply LoRA\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 8,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 2,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 42,\n",
        ")\n",
        "\n",
        "# âœ… 5. Load dataset\n",
        "from datasets import load_dataset\n",
        "train_path = \"/content/drive/MyDrive/ds_6371_qna_500.jsonl\"\n",
        "test_path = \"/content/drive/MyDrive/ds_6371_test_qna_whamo.jsonl\"\n",
        "\n",
        "train_ds = load_dataset(\"json\", data_files={\"train\": train_path}, split=\"train\")\n",
        "test_ds = load_dataset(\"json\", data_files={\"test\": test_path}, split=\"test\")\n",
        "\n",
        "# âœ… 6. Format dataset\n",
        "\n",
        "from unsloth.chat_templates import get_chat_template, standardize_sharegpt\n",
        "\n",
        "# Convert ShareGPT-style to HuggingFace-style\n",
        "train_ds = standardize_sharegpt(train_ds)\n",
        "test_ds = standardize_sharegpt(test_ds)\n",
        "\n",
        "# Apply chat template\n",
        "tokenizer = get_chat_template(tokenizer, chat_template = \"llama-3.1\")\n",
        "\n",
        "def format_prompt(example):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        example[\"conversations\"],\n",
        "        tokenize = False,\n",
        "        add_generation_prompt = False,\n",
        "    )\n",
        "    return { \"text\": text }\n",
        "\n",
        "train_ds = train_ds.map(format_prompt)\n",
        "test_ds = test_ds.map(format_prompt)\n",
        "\n",
        "# âœ… 7. Tokenize dataset\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.1\")\n",
        "\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True)\n",
        "\n",
        "train_ds = train_ds.map(tokenize, remove_columns=train_ds.column_names, num_proc=2)\n",
        "test_inputs = tokenizer(test_ds[\"text\"], return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "# âœ… 8. Train model using TRL\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_ds,\n",
        "    dataset_text_field = \"input_ids\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer),\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 100,  # You can increase this for better results\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# âœ… 9. Mask user input so loss is only calculated on assistant output\n",
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "# âœ… 10. Run inference on test set\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "for i, question in enumerate(test_ds[\"text\"][:5]):\n",
        "    print(f\"--- Test Example {i+1} ---\")\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "    input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "    output = model.generate(input_ids=input_ids, max_new_tokens=64, temperature=0.7)\n",
        "    print(tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yykB0I_vuWZb"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq6rFPjyPhFw"
      },
      "outputs": [],
      "source": [
        "# âœ… Install required packages\n",
        "!pip install -q bert-score sentence-transformers evaluate\n",
        "\n",
        "# âœ… Imports\n",
        "import evaluate\n",
        "from bert_score import score as bert_score\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm import tqdm\n",
        "\n",
        "# âœ… Load evaluation metrics\n",
        "exact_match = evaluate.load(\"exact_match\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "# âœ… Load SentenceTransformer model for cosine similarity\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# âœ… Define predictions and ground truths from your test set\n",
        "preds = []\n",
        "truths = []\n",
        "\n",
        "for example in test_ds:\n",
        "    # Build chat message from test set\n",
        "    messages = example[\"conversations\"]\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generate prediction\n",
        "    output = model.generate(input_ids=input_ids, max_new_tokens=128)\n",
        "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the assistant response\n",
        "    if \"<|assistant|>\" in decoded:\n",
        "        prediction = decoded.split(\"<|assistant|>\")[-1].strip()\n",
        "    else:\n",
        "        prediction = decoded.strip()\n",
        "\n",
        "    preds.append(prediction)\n",
        "\n",
        "    # Get the ground truth answer from test set\n",
        "    for turn in example[\"conversations\"]:\n",
        "        if turn[\"role\"] == \"assistant\":\n",
        "            truths.append(turn[\"content\"].strip())\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "# âœ… Compute Exact Match\n",
        "em_result = exact_match.compute(predictions=preds, references=truths)\n",
        "\n",
        "# âœ… Compute BLEU (over .2 is thought to be okay)\n",
        "bleu_result = bleu.compute(predictions=preds, references=[[t] for t in truths])\n",
        "\n",
        "# âœ… Compute BERTScore\n",
        "\n",
        "P, R, F1 = bert_score(\n",
        "    preds,\n",
        "    truths,\n",
        "    model_type=\"microsoft/deberta-xlarge-mnli\",  # avoids Unsloth conflict\n",
        "    lang=\"en\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "bertscore_result = {\n",
        "    \"precision\": P.mean().item(),\n",
        "    \"recall\": R.mean().item(),\n",
        "    \"f1\": F1.mean().item()\n",
        "}\n",
        "\n",
        "\n",
        "# âœ… Compute cosine similarity using sentence embeddings\n",
        "embedding_similarities = []\n",
        "for pred, truth in zip(preds, truths):\n",
        "    pred_emb = embedder.encode(pred, convert_to_tensor=True)\n",
        "    truth_emb = embedder.encode(truth, convert_to_tensor=True)\n",
        "    sim = util.cos_sim(pred_emb, truth_emb).item()\n",
        "    embedding_similarities.append(sim)\n",
        "\n",
        "cosine_result = {\n",
        "    \"average_cosine_similarity\": sum(embedding_similarities) / len(embedding_similarities)\n",
        "}\n",
        "\n",
        "# âœ… Display results\n",
        "print(\"ðŸ“Š Evaluation Results:\")\n",
        "print(f\"ðŸ”¹ Exact Match: {em_result['exact_match']:.4f}\")\n",
        "print(f\"ðŸ”¹ BLEU Score: {bleu_result['bleu']:.4f}\")\n",
        "print(f\"ðŸ”¹ BERTScore F1: {bertscore_result['f1']:.4f}\")\n",
        "print(f\"ðŸ”¹ Cosine Similarity: {cosine_result['average_cosine_similarity']:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}